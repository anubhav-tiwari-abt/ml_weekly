{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# Developing intuition about large language models.\n","\n","This is a self explainer of Transformer architecture to solidify my understanding of the transformers and large language models.\n","\n","I studied the concepts of the transformers and the attention module from several amazing resources. Each explaining the concepts in a unique and visually interactive and intuitive way. This is kind of a summary of these resources with some of my personal commentry to distill the concept deep in my understanding.\n","\n","<img src=\"resources/intuition_large_language_models/images/large-language-models.jpg\" alt=\"intro image for large language models\" width=\"100%\" height=\"600px\"/>\n","<em>image credit: https://bdtechtalks.com/2021/12/20/artificial-intelligence-large-language-understanding/</em>\n","\n","The amazing resources are : \n","1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n"," \n","2. [Transformers From Scratch](https://e2eml.school/transformers.html) By [Brandon Rohrer](https://twitter.com/_brohrer_)\n","\n","3. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) By Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart and Alexander M. Rush\n","\n","4. [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) By [Amirhossein Kazemnejad](https://kazemnejad.com/)"]},{"cell_type":"markdown","metadata":{},"source":["#### First Let's start from the start\n","\n","A neural network is a connected network of neurons or nodes that take in an input and generate an output, firstly by applying a linear transformation and then optionally applying a non-linearity.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Maintaining the context of the words with recurrence"]},{"cell_type":"markdown","metadata":{},"source":["#### Preserving long range memory using gating"]},{"cell_type":"markdown","metadata":{},"source":["#### What is a transformer anyways?"]},{"cell_type":"markdown","metadata":{},"source":["#### Herein Comes attention"]},{"cell_type":"markdown","metadata":{},"source":["##### Query, Key And Value Embeddings"]},{"cell_type":"markdown","metadata":{},"source":["##### Positional Embeddings"]},{"cell_type":"markdown","metadata":{},"source":["##### One Attention Cycle"]},{"cell_type":"markdown","metadata":{},"source":["##### Cross and Self Attention"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
